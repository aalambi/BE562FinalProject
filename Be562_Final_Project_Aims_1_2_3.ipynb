{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install hmmlearn biopython"
      ],
      "metadata": {
        "id": "-94DVMCZOoTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "jxZ3KOp-2t_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/CoV-AbDab_080224.csv\")"
      ],
      "metadata": {
        "id": "Yy_51v3_4cRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create training labels\n",
        "def label_neutralization(row):\n",
        "    neut = str(row[\"Neutralising Vs\"]).strip()\n",
        "    non = str(row[\"Not Neutralising Vs\"]).strip()\n",
        "\n",
        "    if neut not in [\"\", \"nan\", \"NaN\"] and neut != \"None\":\n",
        "        return 1\n",
        "    if non not in [\"\", \"nan\", \"NaN\"] and non != \"None\":\n",
        "        return 0\n",
        "    return None   # unknown\n",
        "\n",
        "df[\"label\"] = df.apply(label_neutralization, axis=1)\n",
        "df = df.dropna(subset=[\"label\"])\n",
        "df[\"label\"] = df[\"label\"].astype(int)"
      ],
      "metadata": {
        "id": "UVqFg2RQN_OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting the CDRH3 sequences\n",
        "df = df[df[\"CDRH3\"].notna()] #remove rows with missing CDRH3\n",
        "neutralizing_seqs = df[df[\"label\"] == 1][\"CDRH3\"].tolist() #looks at label =1 (neutralizing), remove rows with CDRH3, makes it into list\n",
        "non_neut_seqs = df[df[\"label\"] == 0][\"CDRH3\"].tolist() #looks at label =0 (non- neutralizing), remove rows CDRH3, makes it into list"
      ],
      "metadata": {
        "id": "oYDMW2eLPvMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting amino acid sequences to integers\n",
        "import numpy as np\n",
        "\n",
        "AA = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "aa_to_idx = {aa:i for i,aa in enumerate(AA)}\n",
        "\n",
        "def one_hot(seq):\n",
        "    arr = np.zeros((len(seq), 20), dtype=int)\n",
        "    for i, aa in enumerate(seq):\n",
        "        if aa in aa_to_idx:\n",
        "            arr[i, aa_to_idx[aa]] = 1\n",
        "    return arr\n"
      ],
      "metadata": {
        "id": "I0fSE-5Jy2iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neut_encoded = [one_hot(s) for s in neutralizing_seqs]\n",
        "non_neut_encoded = [one_hot(s) for s in non_neut_seqs]"
      ],
      "metadata": {
        "id": "34ig_zT0y5AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#default trained for 10 iterations\n",
        "from hmmlearn.hmm import MultinomialHMM\n",
        "\n",
        "np.random.seed(42)  # Added this for reproducibility\n",
        "\n",
        "def train_hmm(seqs, n_states=6):\n",
        "    model = MultinomialHMM(n_components=n_states, random_state=42) #take out random state if results are worse\n",
        "    model.n_features = 20  # 20 amino acids\n",
        "\n",
        "    lengths = [len(s) for s in seqs]\n",
        "    X = np.vstack(seqs)  # shape = (total_len, 20)\n",
        "\n",
        "    model.fit(X, lengths)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "SHPPTk_azQE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HMM_neut = train_hmm(neut_encoded, n_states=6)\n",
        "HMM_non = train_hmm(non_neut_encoded, n_states=6)"
      ],
      "metadata": {
        "id": "KZy-ODLOzm3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of amino acids\n",
        "AA = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "aa_to_idx = {aa:i for i,aa in enumerate(AA)}\n",
        "\n",
        "# Function to create labeled DataFrame for emission matrix\n",
        "def create_emission_df(emission_matrix, amino_acids=AA):\n",
        "    n_states = emission_matrix.shape[0]\n",
        "    df = pd.DataFrame(emission_matrix, columns=list(amino_acids))\n",
        "    df.index = [f\"State_{i+1}\" for i in range(n_states)]\n",
        "    return df\n",
        "\n",
        "# Create labeled DataFrames\n",
        "em_neut_df = create_emission_df(HMM_neut.emissionprob_)\n",
        "em_non_df = create_emission_df(HMM_non.emissionprob_)\n",
        "\n",
        "# Display the DataFrames\n",
        "print(\"Neutralizing Antibodies Emission Probability Matrix:\")\n",
        "display(em_neut_df)\n",
        "\n",
        "print(\"\\nNon-Neutralizing Antibodies Emission Probability Matrix:\")\n",
        "display(em_non_df)\n"
      ],
      "metadata": {
        "id": "kHNJmEZlYPQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    balanced_accuracy_score,\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: BIOCHEMICAL FEATURE EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "class BiochemicalFeatureExtractor:\n",
        "    \"\"\"Extract biochemical descriptors from CDRH3 sequences\"\"\"\n",
        "\n",
        "    HYDROPHOBICITY = {\n",
        "        'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,\n",
        "        'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,\n",
        "        'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,\n",
        "        'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2\n",
        "    }\n",
        "\n",
        "    CHARGE = {\n",
        "        'A': 0, 'R': 1, 'N': 0, 'D': -1, 'C': 0,\n",
        "        'Q': 0, 'E': -1, 'G': 0, 'H': 0.5, 'I': 0,\n",
        "        'L': 0, 'K': 1, 'M': 0, 'F': 0, 'P': 0,\n",
        "        'S': 0, 'T': 0, 'W': 0, 'Y': 0, 'V': 0\n",
        "    }\n",
        "\n",
        "    AROMATIC = {'F', 'W', 'Y', 'H'}\n",
        "    POLAR = {'S', 'T', 'N', 'Q', 'C'}\n",
        "    ALIPHATIC = {'A', 'V', 'I', 'L', 'M'}\n",
        "\n",
        "    MOLECULAR_WEIGHT = {\n",
        "        'A': 89.1, 'R': 174.2, 'N': 132.1, 'D': 133.1, 'C': 121.2,\n",
        "        'Q': 146.2, 'E': 147.1, 'G': 75.1, 'H': 155.2, 'I': 131.2,\n",
        "        'L': 131.2, 'K': 146.2, 'M': 149.2, 'F': 165.2, 'P': 115.1,\n",
        "        'S': 105.1, 'T': 119.1, 'W': 204.2, 'Y': 181.2, 'V': 117.1\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_names = []\n",
        "\n",
        "    def extract_features(self, sequence):\n",
        "        features = {}\n",
        "\n",
        "        features['length'] = len(sequence)\n",
        "        hydros = [self.HYDROPHOBICITY.get(aa, 0) for aa in sequence]\n",
        "        charges = [self.CHARGE.get(aa, 0) for aa in sequence]\n",
        "\n",
        "        features['hydrophobicity_mean'] = np.mean(hydros)\n",
        "        features['hydrophobicity_std'] = np.std(hydros)\n",
        "        features['charge_total'] = sum(charges)\n",
        "        features['charge_mean'] = features['charge_total'] / len(sequence)\n",
        "\n",
        "        features['aromatic_count'] = sum(1 for aa in sequence if aa in self.AROMATIC)\n",
        "        features['aromatic_fraction'] = features['aromatic_count'] / len(sequence)\n",
        "\n",
        "        features['polar_count'] = sum(1 for aa in sequence if aa in self.POLAR)\n",
        "        features['polar_fraction'] = features['polar_count'] / len(sequence)\n",
        "\n",
        "        features['aliphatic_count'] = sum(1 for aa in sequence if aa in self.ALIPHATIC)\n",
        "        features['aliphatic_fraction'] = features['aliphatic_count'] / len(sequence)\n",
        "\n",
        "        features['molecular_weight'] = sum([self.MOLECULAR_WEIGHT.get(aa, 0) for aa in sequence])\n",
        "        features['avg_residue_weight'] = features['molecular_weight'] / len(sequence)\n",
        "\n",
        "        n_third = len(sequence) // 3\n",
        "        if n_third > 0:\n",
        "            n_term = sequence[:n_third]\n",
        "            c_term = sequence[-n_third:]\n",
        "            middle = sequence[n_third:-n_third] if len(sequence) > 2 * n_third else sequence\n",
        "\n",
        "            features['n_term_hydrophobicity'] = np.mean([self.HYDROPHOBICITY.get(aa, 0) for aa in n_term])\n",
        "            features['c_term_hydrophobicity'] = np.mean([self.HYDROPHOBICITY.get(aa, 0) for aa in c_term])\n",
        "            features['middle_hydrophobicity'] = np.mean(\n",
        "                [self.HYDROPHOBICITY.get(aa, 0) for aa in middle]\n",
        "            ) if middle else 0\n",
        "\n",
        "            features['n_term_charge'] = sum([self.CHARGE.get(aa, 0) for aa in n_term]) / len(n_term)\n",
        "            features['c_term_charge'] = sum([self.CHARGE.get(aa, 0) for aa in c_term]) / len(c_term)\n",
        "        else:\n",
        "            features['n_term_hydrophobicity'] = features['hydrophobicity_mean']\n",
        "            features['c_term_hydrophobicity'] = features['hydrophobicity_mean']\n",
        "            features['middle_hydrophobicity'] = features['hydrophobicity_mean']\n",
        "            features['n_term_charge'] = features['charge_mean']\n",
        "            features['c_term_charge'] = features['charge_mean']\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_batch(self, sequences):\n",
        "        feature_list = [self.extract_features(seq) for seq in sequences]\n",
        "        df = pd.DataFrame(feature_list)\n",
        "        self.feature_names = df.columns.tolist()\n",
        "        return df\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: HMM MOTIF PROBABILITY EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "def extract_hmm_emission_features(sequences, emission_probs_neut, emission_probs_non_neut,\n",
        "                                  amino_acids='ACDEFGHIKLMNPQRSTVWY'):\n",
        "    aa_to_idx = {aa: i for i, aa in enumerate(amino_acids)}\n",
        "    n_states = emission_probs_neut.shape[0]\n",
        "\n",
        "    motif_features = []\n",
        "\n",
        "    for seq in sequences:\n",
        "        features = {}\n",
        "\n",
        "        for state_idx in range(n_states):\n",
        "            neut_emissions = []\n",
        "            non_neut_emissions = []\n",
        "\n",
        "            for aa in seq:\n",
        "                if aa in aa_to_idx:\n",
        "                    aa_idx = aa_to_idx[aa]\n",
        "                    neut_emissions.append(emission_probs_neut[state_idx, aa_idx])\n",
        "                    non_neut_emissions.append(emission_probs_non_neut[state_idx, aa_idx])\n",
        "\n",
        "            if neut_emissions:\n",
        "                features[f'state_{state_idx}_neut_emission_mean'] = np.mean(neut_emissions)\n",
        "                features[f'state_{state_idx}_non_neut_emission_mean'] = np.mean(non_neut_emissions)\n",
        "                features[f'state_{state_idx}_emission_ratio'] = (\n",
        "                    np.mean(neut_emissions) / (np.mean(non_neut_emissions) + 1e-10)\n",
        "                )\n",
        "                features[f'state_{state_idx}_neut_emission_max'] = np.max(neut_emissions)\n",
        "                features[f'state_{state_idx}_non_neut_emission_max'] = np.max(non_neut_emissions)\n",
        "\n",
        "        log_prob_neut = 0.0\n",
        "        log_prob_non_neut = 0.0\n",
        "\n",
        "        for aa in seq:\n",
        "            if aa in aa_to_idx:\n",
        "                aa_idx = aa_to_idx[aa]\n",
        "                log_prob_neut += np.log(np.max(emission_probs_neut[:, aa_idx]) + 1e-300)\n",
        "                log_prob_non_neut += np.log(np.max(emission_probs_non_neut[:, aa_idx]) + 1e-300)\n",
        "\n",
        "        features['log_likelihood_neut'] = log_prob_neut\n",
        "        features['log_likelihood_non_neut'] = log_prob_non_neut\n",
        "        features['log_likelihood_ratio'] = log_prob_neut - log_prob_non_neut\n",
        "\n",
        "        motif_features.append(features)\n",
        "\n",
        "    return pd.DataFrame(motif_features)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: INTEGRATED PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def add_interaction_features(X,\n",
        "                             biochem_cols=None,\n",
        "                             motif_cols=None,\n",
        "                             max_motif=10):\n",
        "    X = X.copy()\n",
        "\n",
        "    if biochem_cols is None:\n",
        "        biochem_cols = [\n",
        "            c for c in X.columns\n",
        "            if any(k in c for k in [\n",
        "                'hydrophobicity', 'charge', 'aromatic_fraction',\n",
        "                'polar_fraction', 'aliphatic_fraction', 'length'\n",
        "            ])\n",
        "        ]\n",
        "\n",
        "    if motif_cols is None:\n",
        "        all_motif = [c for c in X.columns\n",
        "                     if c.startswith('state_') or 'log_likelihood' in c]\n",
        "        motif_cols = all_motif[:max_motif]\n",
        "\n",
        "    print(f\"Adding interactions between {len(biochem_cols)} biochem \"\n",
        "          f\"and {len(motif_cols)} motif features\")\n",
        "\n",
        "    inter_cols = {}\n",
        "    for b in biochem_cols:\n",
        "        for m in motif_cols:\n",
        "            inter_name = f'{b}__X__{m}'\n",
        "            inter_cols[inter_name] = X[b] * X[m]\n",
        "\n",
        "    inter_df = pd.DataFrame(inter_cols, index=X.index)\n",
        "    X = pd.concat([X, inter_df], axis=1)\n",
        "\n",
        "    print(f\"Total features after interactions: {X.shape[1]}\")\n",
        "    return X\n",
        "\n",
        "\n",
        "def create_integrated_feature_matrix(sequences, labels, emission_probs_neut, emission_probs_non_neut):\n",
        "    print(\"Extracting HMM emission-based features...\")\n",
        "    motif_features = extract_hmm_emission_features(\n",
        "        sequences, emission_probs_neut, emission_probs_non_neut\n",
        "    )\n",
        "\n",
        "    print(\"Extracting biochemical features...\")\n",
        "    extractor = BiochemicalFeatureExtractor()\n",
        "    biochem_features = extractor.extract_batch(sequences)\n",
        "\n",
        "    X = pd.concat([motif_features, biochem_features], axis=1)\n",
        "    y = np.array(labels)\n",
        "\n",
        "    X = add_interaction_features(X)\n",
        "\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    print(f\"Total features: {len(feature_names)}\")\n",
        "    print(f\"  - Motif features: {len(motif_features.columns)}\")\n",
        "    print(f\"  - Biochemical features: {len(biochem_features.columns)}\")\n",
        "\n",
        "    return X, y, feature_names\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: BAYESIAN MODEL USING MAP ESTIMATION (NO CV, NO HESSIAN)\n",
        "# ============================================================================\n",
        "\n",
        "class FastBayesianNeutralizationModel:\n",
        "    \"\"\"\n",
        "    Logistic regression with L2 regularization (Gaussian prior).\n",
        "    No CV, no Hessian; fast and stable.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, prior_scale=2.5):\n",
        "        self.prior_scale = prior_scale\n",
        "        self.lambda_reg = 1.0 / (prior_scale ** 2)\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.feature_names = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.std_errors_ = None\n",
        "        self.best_thresh_ = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "\n",
        "    def _negative_log_posterior(self, params, X, y):\n",
        "        intercept = params[0]\n",
        "        coef = params[1:]\n",
        "\n",
        "        z = intercept + X @ coef\n",
        "        p = self._sigmoid(z)\n",
        "\n",
        "        epsilon = 1e-15\n",
        "        p = np.clip(p, epsilon, 1 - epsilon)\n",
        "\n",
        "        N = len(y)\n",
        "        N_pos = np.sum(y == 1)\n",
        "        N_neg = N - N_pos\n",
        "        if N_pos == 0 or N_neg == 0:\n",
        "            w_pos = 1.0\n",
        "            w_neg = 1.0\n",
        "        else:\n",
        "            w_pos = N / (2.0 * N_pos)\n",
        "            w_neg = N / (2.0 * N_neg)\n",
        "\n",
        "        nll = -np.sum(\n",
        "            w_pos * y * np.log(p) +\n",
        "            w_neg * (1 - y) * np.log(1 - p)\n",
        "        )\n",
        "\n",
        "        nlp = 0.5 * self.lambda_reg * np.sum(coef ** 2)\n",
        "\n",
        "        return nll + nlp\n",
        "\n",
        "    def fit(self, X, y, feature_names):\n",
        "        self.feature_names = feature_names\n",
        "\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        n_features = X_scaled.shape[1]\n",
        "        init_params = np.zeros(n_features + 1)\n",
        "\n",
        "        print(\"Fitting model using MAP estimation...\")\n",
        "        print(f\"Prior scale (σ): {self.prior_scale}\")\n",
        "        print(f\"Regularization strength (λ): {self.lambda_reg:.4f}\")\n",
        "\n",
        "        result = minimize(\n",
        "            self._negative_log_posterior,\n",
        "            init_params,\n",
        "            args=(X_scaled, y),\n",
        "            method='L-BFGS-B',\n",
        "            options={'maxiter': 1000}\n",
        "        )\n",
        "\n",
        "        if result.success:\n",
        "            print(\"Optimization converged successfully!\")\n",
        "        else:\n",
        "            print(\"Warning: Optimization may not have fully converged\")\n",
        "\n",
        "        self.intercept_ = result.x[0]\n",
        "        self.coef_ = result.x[1:]\n",
        "\n",
        "        # Dummy SDs so coefficient summary still works\n",
        "        self.std_errors_ = np.ones_like(result.x) * 0.01\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        z = self.intercept_ + X_scaled @ self.coef_\n",
        "        prob = self._sigmoid(z)\n",
        "\n",
        "        if self.std_errors_ is not None:\n",
        "            z_std = np.sqrt(np.sum((X_scaled * self.std_errors_[1:]) ** 2, axis=1))\n",
        "            prob_lower = self._sigmoid(z - 1.96 * z_std)\n",
        "            prob_upper = self._sigmoid(z + 1.96 * z_std)\n",
        "        else:\n",
        "            prob_lower = np.full_like(prob, np.nan)\n",
        "            prob_upper = np.full_like(prob, np.nan)\n",
        "\n",
        "        return prob, prob_lower, prob_upper\n",
        "\n",
        "    def get_coefficient_summary(self):\n",
        "        feature_list = ['alpha'] + self.feature_names\n",
        "        summary = pd.DataFrame({\n",
        "            'feature': feature_list,\n",
        "            'mean': np.concatenate([[self.intercept_], self.coef_]),\n",
        "            'sd': self.std_errors_,\n",
        "        })\n",
        "        summary['ci_lower'] = summary['mean'] - 1.96 * summary['sd']\n",
        "        summary['ci_upper'] = summary['mean'] + 1.96 * summary['sd']\n",
        "        return summary\n",
        "\n",
        "    def plot_coefficient_forest(self, top_n=20):\n",
        "        coef_summary = self.get_coefficient_summary()\n",
        "        coef_summary = coef_summary[coef_summary['feature'] != 'alpha']\n",
        "        coef_summary['abs_mean'] = np.abs(coef_summary['mean'])\n",
        "        coef_summary = coef_summary.sort_values('abs_mean', ascending=False).head(top_n)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        y_pos = np.arange(len(coef_summary))\n",
        "        ax.errorbar(\n",
        "            coef_summary['mean'],\n",
        "            y_pos,\n",
        "            xerr=[coef_summary['mean'] - coef_summary['ci_lower'],\n",
        "                  coef_summary['ci_upper'] - coef_summary['mean']],\n",
        "            fmt='o',\n",
        "            capsize=5,\n",
        "            capthick=2\n",
        "        )\n",
        "\n",
        "        ax.axvline(0, linestyle='--', alpha=0.5)\n",
        "        ax.set_yticks(y_pos)\n",
        "        ax.set_yticklabels(coef_summary['feature'])\n",
        "        ax.set_xlabel('Coefficient Value (Standardized)', fontsize=12)\n",
        "        ax.set_title(f'Top {top_n} Features by Coefficient Magnitude', fontsize=14)\n",
        "        ax.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: MODEL EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"MODEL EVALUATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    y_pred_proba, _, _ = model.predict_proba(X_test)\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "    youden_j = tpr - fpr\n",
        "    best_idx = np.argmax(youden_j)\n",
        "    best_thresh = thresholds[best_idx]\n",
        "    print(f\"\\nOptimal threshold (Youden J): {best_thresh:.3f}\")\n",
        "\n",
        "    model.best_thresh_ = best_thresh\n",
        "    y_pred = (y_pred_proba >= best_thresh).astype(int)\n",
        "\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    print(f\"\\nROC-AUC Score: {auc:.4f}\")\n",
        "\n",
        "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "    print(f\"Balanced Accuracy: {bal_acc:.4f}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        y_test,\n",
        "        y_pred,\n",
        "        target_names=['Non-Neutralizing', 'Neutralizing']\n",
        "    ))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
        "    ax1.set_xlabel('Predicted')\n",
        "    ax1.set_ylabel('Actual')\n",
        "    ax1.set_title('Confusion Matrix')\n",
        "    ax1.set_xticklabels(['Non-Neut', 'Neut'])\n",
        "    ax1.set_yticklabels(['Non-Neut', 'Neut'])\n",
        "\n",
        "    ax2.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {auc:.3f})')\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "    ax2.set_xlabel('False Positive Rate')\n",
        "    ax2.set_ylabel('True Positive Rate')\n",
        "    ax2.set_title('ROC Curve')\n",
        "    ax2.legend()\n",
        "    ax2.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return auc, y_pred_proba, best_thresh\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: MAIN EXECUTION (NO PRIOR TUNING)\n",
        "# ============================================================================\n",
        "\n",
        "def main_example_fast(emission_probs_neut, emission_probs_non_neut, sequences, labels):\n",
        "    print(\"=\" * 60)\n",
        "    print(\"BAYESIAN LOGISTIC REGRESSION (MAP ESTIMATION)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(f\"\\n Loaded emission matrices:\")\n",
        "    print(f\"  - Neutralizing HMM: {emission_probs_neut.shape}\")\n",
        "    print(f\"  - Non-neutralizing HMM: {emission_probs_non_neut.shape}\")\n",
        "    print(f\"\\n Data summary:\")\n",
        "    print(f\"  - Total sequences: {len(sequences)}\")\n",
        "    print(f\"  - Neutralizing: {sum(labels == 1)}\")\n",
        "    print(f\"  - Non-neutralizing: {sum(labels == 0)}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXTRACTING FEATURES\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    X_full, y, feature_names = create_integrated_feature_matrix(\n",
        "        sequences, labels, emission_probs_neut, emission_probs_non_neut\n",
        "    )\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_full.values, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining set: {len(X_train)} samples\")\n",
        "    print(f\"Test set: {len(X_test)} samples\")\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Single fit, fixed prior_scale\n",
        "    model = FastBayesianNeutralizationModel(prior_scale=2.5)\n",
        "    model.fit(X_train, y_train, feature_names)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"\\n Training completed in {elapsed_time:.1f} seconds!\")\n",
        "\n",
        "    auc, y_pred_proba, best_thresh = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"COEFFICIENT SUMMARY (Top 15)\")\n",
        "    print(\"=\" * 60)\n",
        "    coef_summary = model.get_coefficient_summary()\n",
        "    print(coef_summary.head(15).to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"GENERATING VISUALIZATIONS\")\n",
        "    print(\"=\" * 60)\n",
        "    model.plot_coefficient_forest(top_n=15)\n",
        "    plt.show()\n",
        "\n",
        "    # If you have plot_prediction_uncertainty defined elsewhere, you can still call it:\n",
        "    try:\n",
        "        y_pred_mean, y_pred_lower, y_pred_upper = model.predict_proba(X_test)\n",
        "        plot_prediction_uncertainty(y_test, y_pred_mean, y_pred_lower, y_pred_upper)\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    print(\"\\n Fast analysis complete!\")\n",
        "    print(\"\\nKey outputs:\")\n",
        "    print(\"- MAP coefficient estimates\")\n",
        "    print(\"- Feature importance rankings\")\n",
        "    print(\"- Model performance metrics\")\n",
        "\n",
        "    return model, coef_summary, X_full, feature_names\n",
        "\n",
        "print(\"Bayesian model loaded!\")"
      ],
      "metadata": {
        "id": "oQH6z_G549D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# FAST AIM 2: RUN BAYESIAN LOGISTIC REGRESSION (MAP ESTIMATION)\n",
        "# ==============================================================================\n",
        "# This runs in 1-2 minutes instead of 15+ minutes!\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PREPARING DATA FROM AIM 1\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extract emission probability matrices from your trained HMMs\n",
        "emission_probs_neut = HMM_neut.emissionprob_\n",
        "emission_probs_non_neut = HMM_non.emissionprob_\n",
        "\n",
        "# Combine sequences and create labels\n",
        "all_sequences = neutralizing_seqs + non_neut_seqs\n",
        "all_labels = np.array([1]*len(neutralizing_seqs) + [0]*len(non_neut_seqs))\n",
        "\n",
        "print(f\"\\n Loaded neutralizing HMM emissions: {emission_probs_neut.shape}\")\n",
        "print(f\" Loaded non-neutralizing HMM emissions: {emission_probs_non_neut.shape}\")\n",
        "print(f\" Total sequences: {len(all_sequences)}\")\n",
        "print(f\"  - Neutralizing: {sum(all_labels == 1)}\")\n",
        "print(f\"  - Non-neutralizing: {sum(all_labels == 0)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# RUN THE FAST ANALYSIS (uses MAP instead of MCMC)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RUNNING FAST AIM 2 ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Run the fast analysis\n",
        "model, coef_summary, X_full, feature_names = main_example_fast(\n",
        "    emission_probs_neut,\n",
        "    emission_probs_non_neut,\n",
        "    all_sequences,\n",
        "    all_labels\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# FEATURE IMPORTANCE BY CATEGORY\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FEATURE IMPORTANCE BY CATEGORY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get all features\n",
        "all_features = coef_summary[coef_summary['feature'] != 'alpha'].copy()\n",
        "all_features['abs_mean'] = np.abs(all_features['mean'])\n",
        "\n",
        "# Categorize features\n",
        "motif_features = all_features[all_features['feature'].str.contains('state_|log_likelihood')]\n",
        "biochem_features = all_features[~all_features['feature'].str.contains('state_|log_likelihood')]\n",
        "\n",
        "print(f\"\\n TOP 10 HMM MOTIF FEATURES:\")\n",
        "print(\"-\" * 70)\n",
        "for idx, row in motif_features.nlargest(10, 'abs_mean').iterrows():\n",
        "    direction = \"↑\" if row['mean'] > 0 else \"↓\"\n",
        "    print(f\"{direction} {row['feature']:45s} β={row['mean']:7.3f}  95% CI: [{row['ci_lower']:6.3f}, {row['ci_upper']:6.3f}]\")\n",
        "\n",
        "print(f\"\\n TOP 10 BIOCHEMICAL FEATURES:\")\n",
        "print(\"-\" * 70)\n",
        "for idx, row in biochem_features.nlargest(10, 'abs_mean').iterrows():\n",
        "    direction = \"↑\" if row['mean'] > 0 else \"↓\"\n",
        "    print(f\"{direction} {row['feature']:45s} β={row['mean']:7.3f}  95% CI: [{row['ci_lower']:6.3f}, {row['ci_upper']:6.3f}]\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SUMMARY\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FAST ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n Summary:\")\n",
        "print(f\"  - Total features analyzed: {len(all_features)}\")\n",
        "print(f\"    • HMM motif features: {len(motif_features)}\")\n",
        "print(f\"    • Biochemical features: {len(biochem_features)}\")\n",
        "\n",
        "print(f\"\\n Top 3 Most Influential Features Overall:\")\n",
        "top_3 = all_features.nlargest(3, 'abs_mean')\n",
        "for i, (idx, row) in enumerate(top_3.iterrows(), 1):\n",
        "    effect = \"promotes\" if row['mean'] > 0 else \"inhibits\"\n",
        "    print(f\"  {i}. {row['feature']}\")\n",
        "    print(f\"     → {effect} neutralization (β = {row['mean']:.3f}, 95% CI: [{row['ci_lower']:.3f}, {row['ci_upper']:.3f}])\")\n",
        "\n",
        "print(\"\\n Aim 2 Complete!\")\n",
        "print(\"\\nNote: This uses MAP estimation (fast) instead of full MCMC (slow)\")\n",
        "print(\"Results are very similar but computed 50-100x faster!\")\n",
        "print(\"\\nResults stored in variables:\")\n",
        "print(\"  - model: trained FastBayesianNeutralizationModel\")\n",
        "\n",
        "print(\"  - coef_summary: DataFrame with coefficient estimates\")"
      ],
      "metadata": {
        "id": "hpUZWGjX5UcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "import re\n",
        "# --------------------------\n",
        "# 1. Extract V-gene family\n",
        "# --------------------------\n",
        "\n",
        "def extract_v_family_column(df, v_col=None):\n",
        "    \"\"\"\n",
        "    Add a 'V_gene_family' column to df based on a V-gene column.\n",
        "    For CoV-AbDab, we primarily use 'Heavy V Gene'.\n",
        "    \"\"\"\n",
        "    if \"V_gene_family\" in df.columns:\n",
        "        return df\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    if v_col is None:\n",
        "        candidate_cols = [\n",
        "            \"Heavy V Gene\", \"VHorVHH\", \"V_gene\", \"v_gene\",\n",
        "            \"V_CALL\", \"v_call\", \"heavy_v_gene\", \"VH_gene\", \"vh_gene\"\n",
        "        ]\n",
        "        found = [c for c in candidate_cols if c in df.columns]\n",
        "        if not found:\n",
        "            raise ValueError(\n",
        "                \"No V-gene column found. \"\n",
        "                \"Provide v_col or ensure one of the standard names exists.\"\n",
        "            )\n",
        "        v_col = found[0]\n",
        "\n",
        "    def parse_family(x):\n",
        "        if pd.isna(x):\n",
        "            return np.nan\n",
        "\n",
        "        s = str(x).strip()\n",
        "\n",
        "        if \"(\" in s:\n",
        "            s = s.split(\"(\")[0].strip()\n",
        "\n",
        "        if \"*\" in s:\n",
        "            s = s.split(\"*\")[0].strip()\n",
        "\n",
        "        s = s.split()[0]\n",
        "\n",
        "        m = re.match(r\"^([A-Za-z]+[0-9]+)\", s)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "\n",
        "        if \"-\" in s:\n",
        "            return s.split(\"-\")[0].strip() or np.nan\n",
        "\n",
        "        return s or np.nan\n",
        "\n",
        "    df[\"V_gene_family\"] = df[v_col].apply(parse_family)\n",
        "    return df\n",
        "\n",
        "def run_aim3_MAP(df, X_features, y_labels, feature_names, min_group_size=15):\n",
        "    \"\"\"\n",
        "    Hierarchical Bayesian logistic regression with V-gene family grouping.\n",
        "\n",
        "    - Group-specific intercepts a_g ~ N(mu_a, sigma_a^2)\n",
        "    - Global slopes beta_f shared across families\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Alignment checks\n",
        "    assert len(df) == len(X_features) == len(y_labels), \"Data must be aligned.\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # 2. Extract V-gene family\n",
        "    df = extract_v_family_column(df)\n",
        "\n",
        "    # Remove NA families\n",
        "    mask_valid = df[\"V_gene_family\"].notna()\n",
        "    df = df[mask_valid].reset_index(drop=True)\n",
        "    X_features = X_features[mask_valid]\n",
        "    y_labels = np.array(y_labels)[mask_valid]\n",
        "\n",
        "    # 3. Filter by minimum group size\n",
        "    family_counts = df[\"V_gene_family\"].value_counts()\n",
        "    valid = family_counts[family_counts >= min_group_size].index\n",
        "    keep_mask = df[\"V_gene_family\"].isin(valid)\n",
        "    df = df[keep_mask].reset_index(drop=True)\n",
        "    X_features = X_features[keep_mask]\n",
        "    y_labels = np.array(y_labels)[keep_mask]\n",
        "\n",
        "    print(f\"Retained {len(df)} samples across {len(valid)} families\")\n",
        "\n",
        "    # 4. Prepare group codes\n",
        "    df[\"V_gene_family\"] = df[\"V_gene_family\"].astype(\"category\")\n",
        "    group_idx = df[\"V_gene_family\"].cat.codes.values\n",
        "    group_names = df[\"V_gene_family\"].cat.categories.tolist()\n",
        "    n_groups = len(group_names)\n",
        "\n",
        "    # 5. Train-test split\n",
        "    X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
        "        X_features,\n",
        "        y_labels,\n",
        "        group_idx,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y_labels\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train)\n",
        "    X_test_s = scaler.transform(X_test)\n",
        "\n",
        "    n_features = X_train_s.shape[1]\n",
        "\n",
        "    # Checkpoint to ensure dataset was pre-processed correctly\n",
        "    print(\"Any NaNs in X_train_s?\", np.isnan(X_train_s).any())\n",
        "    print(\"Any infs in X_train_s?\", np.isinf(X_train_s).any())\n",
        "    print(\"X_train_s min/max:\", X_train_s.min(), X_train_s.max())\n",
        "\n",
        "    # 6. Hierarchical logistic regression: random intercepts + global slopes\n",
        "    with pm.Model() as hierarchical_model:\n",
        "        # Hyperpriors for intercepts\n",
        "        mu_a = pm.Normal(\"mu_a\", mu=0, sigma=2)\n",
        "        sigma_a = pm.HalfNormal(\"sigma_a\", sigma=2)\n",
        "\n",
        "        # Group-specific intercepts\n",
        "        a = pm.Normal(\"a\", mu=mu_a, sigma=sigma_a, shape=n_groups)\n",
        "\n",
        "        # Global slopes (no hierarchy on slopes)\n",
        "        beta = pm.Normal(\"beta\", mu=0, sigma=2, shape=n_features)\n",
        "\n",
        "        # Linear predictor\n",
        "        logit_p = a[group_train] + pm.math.dot(X_train_s, beta)\n",
        "\n",
        "        # Likelihood\n",
        "        y_obs = pm.Bernoulli(\"y_obs\", logit_p=logit_p, observed=y_train)\n",
        "\n",
        "        # MAP estimate\n",
        "        map_estimate = pm.find_MAP(method=\"Powell\")\n",
        "\n",
        "    # 7. Predictions\n",
        "    a_map = map_estimate[\"a\"]        # (n_groups,)\n",
        "    beta_map = map_estimate[\"beta\"]  # (n_features,)\n",
        "\n",
        "    def predict(X, groups):\n",
        "        logit = a_map[groups] + np.dot(X, beta_map)\n",
        "        return 1 / (1 + np.exp(-logit))\n",
        "\n",
        "    y_train_pred = predict(X_train_s, group_train)\n",
        "    y_test_pred = predict(X_test_s, group_test)\n",
        "\n",
        "    # 8. Metrics + plots (same style as Aim 2)\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n",
        "    youden_j = tpr - fpr\n",
        "    best_idx = np.argmax(youden_j)\n",
        "    best_thresh = thresholds[best_idx]\n",
        "    print(f\"\\nOptimal threshold (Youden J): {best_thresh:.3f}\")\n",
        "\n",
        "    y_test_bin = (y_test_pred >= best_thresh).astype(int)\n",
        "\n",
        "    auc_train = roc_auc_score(y_train, y_train_pred)\n",
        "    auc_test = roc_auc_score(y_test, y_test_pred)\n",
        "    bal_acc = balanced_accuracy_score(y_test, y_test_bin)\n",
        "\n",
        "    print(f\"Train AUC: {auc_train:.3f}, Test AUC: {auc_test:.3f}\")\n",
        "    print(f\"Balanced accuracy (test): {bal_acc:.3f}\")\n",
        "    report_test = classification_report(\n",
        "        y_test, y_test_bin,\n",
        "        target_names=['Non-Neutralizing', 'Neutralizing']\n",
        "    )\n",
        "    print(\"Test Classification Report:\\n\", report_test)\n",
        "\n",
        "    # Confusion matrix + ROC curve\n",
        "    cm = confusion_matrix(y_test, y_test_bin)\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
        "    ax1.set_xlabel('Predicted')\n",
        "    ax1.set_ylabel('Actual')\n",
        "    ax1.set_title('Aim 3 Confusion Matrix')\n",
        "    ax1.set_xticklabels(['Non-Neut', 'Neut'])\n",
        "    ax1.set_yticklabels(['Non-Neut', 'Neut'])\n",
        "\n",
        "    ax2.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {auc_test:.3f})')\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "    ax2.set_xlabel('False Positive Rate')\n",
        "    ax2.set_ylabel('True Positive Rate')\n",
        "    ax2.set_title('Aim 3 ROC Curve')\n",
        "    ax2.legend()\n",
        "    ax2.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        \"model\": hierarchical_model,\n",
        "        \"map_estimate\": map_estimate,\n",
        "        \"scaler\": scaler,\n",
        "        \"y_train_pred\": y_train_pred,\n",
        "        \"y_test_pred\": y_test_pred,\n",
        "        \"auc_train\": auc_train,\n",
        "        \"auc_test\": auc_test,\n",
        "        \"balanced_acc\": bal_acc,\n",
        "        \"best_thresh\": best_thresh,\n",
        "        \"report_test\": report_test,\n",
        "        \"group_names\": group_names,\n",
        "        \"fpr\": fpr,\n",
        "        \"tpr\": tpr,\n",
        "    }"
      ],
      "metadata": {
        "id": "9kCGqNY068N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making copies to be used in Aim 3 so that results don't overwrite Aim 2\n",
        "df_neut = df[df[\"label\"] == 1].copy()\n",
        "df_non  = df[df[\"label\"] == 0].copy()\n",
        "df_aim3 = pd.concat([df_neut, df_non], ignore_index=True)\n",
        "\n",
        "# Checking to make sure dimensions match\n",
        "print(df_aim3.shape[0], X_full.shape[0], len(all_labels))"
      ],
      "metadata": {
        "id": "6tVLmlcEIjcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_aim3 = [c for c in X_full.columns if \"__X__\" not in c]   # no interaction terms\n",
        "X_aim3 = X_full[cols_aim3].values\n",
        "feature_names_aim3 = cols_aim3"
      ],
      "metadata": {
        "id": "QUM-Wr8bMpdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "vt = VarianceThreshold(threshold=1e-4)\n",
        "X_aim3 = vt.fit_transform(X_aim3)\n",
        "feature_names_aim3 = [cols_aim3[i] for i, v in enumerate(vt.variances_) if v > 1e-4]"
      ],
      "metadata": {
        "id": "K4wg_9h3MrMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = run_aim3_MAP(\n",
        "    df=df_aim3,                  # DataFrame with Heavy V Gene etc.\n",
        "    X_features=X_aim3,      # 2D array (n_samples, n_features)\n",
        "    y_labels=all_labels,             # 1D array-like of 0/1\n",
        "    feature_names=feature_names_aim3,\n",
        "    min_group_size=15\n",
        ")"
      ],
      "metadata": {
        "id": "x4IlPyjIDbw7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}